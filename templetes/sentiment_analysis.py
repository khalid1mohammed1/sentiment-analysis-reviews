# -*- coding: utf-8 -*-
"""sentiment_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rTaEwSPtqAp6yxrGXS8HeTIP2gW9Mlgu
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from nltk.stem.porter import PorterStemmer
nltk.download('stopwords')
from nltk.corpus import stopwords
STOPWORDS = set(stopwords.words('english'))
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score
from wordcloud import WordCloud
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
import pickle
import re

# Exploratory Data Analysis

#Load the data
data = pd.read_csv("/content/amazon_alexa.tsv", delimiter = '\t', quoting = 3)
data=pd.DataFrame(data)
print(f"Dataset shape : {data.shape}")
print(data.head())
data

#Column names
print(f"Feature names : {data.columns.values}")

#Check for null values
print(data.isnull().sum())

#Getting the record where 'verified_reviews' is null
print(data[data['verified_reviews'].isna() == True])

#We will drop the null record
data.dropna(inplace=True)
print(f"Dataset shape after dropping null values : {data.shape}")

#Creating a new column 'length' that will contain the length of the string in 'verified_reviews' column
data['length'] = data['verified_reviews'].apply(len)
print(data.head())

#Randomly checking for 10th record
print(f"'verified_reviews' column value: {data.iloc[10]['verified_reviews']}") #Original value
print(f"Length of review : {len(data.iloc[10]['verified_reviews'])}") #Length of review using len()
print(f"'length' column value : {data.iloc[10]['length']}") #Value of the column 'length'
print("................................")
print(data.dtypes)
print("................................")
print(len(data))

#Distinct values of 'rating' and its count
print(f"Rating value count: \n{data['rating'].value_counts()}")

#Bar plot to visualize the total counts of each rating
data['rating'].value_counts().plot.bar(color = 'red')
plt.title('Rating distribution count')
plt.xlabel('Ratings')
plt.ylabel('Count')
plt.show()

#Finding the percentage distribution of each rating - we'll divide the number of records for each rating by total number of records

print(f"Rating value count - percentage distribution: \n{round(data['rating'].value_counts()/data.shape[0]*100,2)}")

fig = plt.figure(figsize=(7,7))
colors = ('red', 'green', 'blue','orange','yellow')
wp = {'linewidth':1, "edgecolor":'black'}
tags = data['rating'].value_counts()/data.shape[0]
explode=(0.1,0.1,0.1,0.1,0.1)
tags.plot(kind='pie', autopct="%1.1f%%", shadow=True, colors=colors, startangle=90, wedgeprops=wp, explode=explode, label='Percentage wise distrubution of rating')
from io import  BytesIO
graph = BytesIO()
fig.savefig(graph, format="png")

#Distinct values of 'feedback' and its count
print(f"Feedback value count: \n{data['feedback'].value_counts()}")

#Extracting the 'verified_reviews' value for one record with feedback = 0
review_0 = data[data['feedback'] == 0].iloc[1]['verified_reviews']
print(review_0)

#Extracting the 'verified_reviews' value for one record with feedback = 1
review_1 = data[data['feedback'] == 1].iloc[1]['verified_reviews']
print(review_1)

"""From the above 2 examples we can see that feedback 0 is negative review and 1 is positive **review**"""

#Bar graph to visualize the total counts of each feedback
data['feedback'].value_counts().plot.bar(color = 'blue')
plt.title('Feedback distribution count')
plt.xlabel('Feedback')
plt.ylabel('Count')
plt.show()

#Finding the percentage distribution of each feedback - we'll divide the number of records for each feedback by total number of records
print(f"Feedback value count - percentage distribution: \n{round(data['feedback'].value_counts()/data.shape[0]*100,2)}")

"""-Feedback value count - percentage distribution:
* 1    91.84
* 0     8.16
-Name: feedback, dtype: float64
-Feedback distribution

* =>91.87% reviews are positive
* =>8.13% reviews are negative
"""

fig = plt.figure(figsize=(7,7))
colors = ('red', 'green')
wp = {'linewidth':1, "edgecolor":'black'}
tags = data['feedback'].value_counts()/data.shape[0]
explode=(0.1,0.1)
tags.plot(kind='pie', autopct="%1.1f%%", shadow=True, colors=colors, startangle=90, wedgeprops=wp, explode=explode, label='Percentage wise distrubution of feedback')

#Feedback = 0
data[data['feedback'] == 0]['rating'].value_counts()

#Feedback = 1
data[data['feedback'] == 1]['rating'].value_counts()

#Distinct values of 'variation' and its count
print(f"Variation value count: \n{data['variation'].value_counts()}")

#Bar graph to visualize the total counts of each variation

data['variation'].value_counts().plot.bar(color = 'orange')
plt.title('Variation distribution count')
plt.xlabel('Variation')
plt.ylabel('Count')
plt.show()

#Finding the percentage distribution of each variation - we'll divide the number of records for each variation by total number of records

print(f"Variation value count - percentage distribution: \n{round(data['variation'].value_counts()/data.shape[0]*100,2)}")

data.groupby('variation')['rating'].mean()

data.groupby('variation')['rating'].mean().sort_values().plot.bar(color = 'brown', figsize=(11, 6))
plt.title("Mean rating according to variation")
plt.xlabel('Variation')
plt.ylabel('Mean rating')
plt.show()

data['length'].describe()

sns.histplot(data['length'],color='blue').set(title='Distribution of length of review ')

sns.histplot(data[data['feedback']==0]['length'],color='red').set(title='Distribution of length of review if feedback = 0')

sns.histplot(data[data['feedback']==1]['length'],color='green').set(title='Distribution of length of review if feedback = 1')

data.groupby('length')['rating'].mean().plot.hist(color = 'blue', figsize=(7, 6), bins = 20)
plt.title(" Review length wise mean ratings")
plt.xlabel('ratings')
plt.ylabel('length')
plt.show()

cv = CountVectorizer(stop_words='english')
words = cv.fit_transform(data.verified_reviews)

# Combine all reviews
reviews = " ".join([review for review in data['verified_reviews']])

# Initialize wordcloud object
wc = WordCloud(background_color='black', max_words=50)

# Generate and plot wordcloud
plt.figure(figsize=(10,10))
plt.imshow(wc.generate(reviews))
plt.title('Wordcloud for all reviews', fontsize=10)
plt.axis('off')
plt.show()

# Combine all reviews for each feedback category and splitting them into individual words
neg_reviews = " ".join([review for review in data[data['feedback'] == 0]['verified_reviews']])
neg_reviews = neg_reviews.lower().split()

pos_reviews = " ".join([review for review in data[data['feedback'] == 1]['verified_reviews']])
pos_reviews = pos_reviews.lower().split()

#Finding words from reviews which are present in that feedback category only
unique_negative = [x for x in neg_reviews if x not in pos_reviews]
unique_negative = " ".join(unique_negative)

unique_positive = [x for x in pos_reviews if x not in neg_reviews]
unique_positive = " ".join(unique_positive)

wc = WordCloud(background_color='white', max_words=50)
# Generate and plot wordcloud
plt.figure(figsize=(10,10))
plt.imshow(wc.generate(unique_negative))
plt.title('Wordcloud for negative reviews', fontsize=10)
plt.axis('off')
plt.show()

wc = WordCloud(background_color='white', max_words=50)
# Generate and plot wordcloud
plt.figure(figsize=(10,10))
plt.imshow(wc.generate(unique_positive))
plt.title('Wordcloud for positive reviews', fontsize=10)
plt.axis('off')
plt.show()

"""-Preprocessing and Modelling
To build the corpus from the 'verified_reviews' we perform the following -

1)Replace any non alphabet characters with a space

2)Covert to lower case and split into words

3)Iterate over the individual words and if it is not a stopword then add the original form of the word to the corpus
"""

corpus = []
stemmer = PorterStemmer()
for i in range(0, data.shape[0]):
  review = re.sub('[^a-zA-Z]', ' ', data.iloc[i]['verified_reviews'])
  review = review.lower().split()
  review = [stemmer.stem(word) for word in review if not word in STOPWORDS]
  review = ' '.join(review)
  corpus.append(review)

cv = CountVectorizer(max_features = 2500)
#Storing independent and dependent variables in X and y
X = cv.fit_transform(corpus).toarray()
y = data['feedback'].values
cv

#Saving the Count Vectorizer
pickle.dump(cv, open('countVectorizer.pkl', 'wb'))

print(f"X shape: {X.shape}")
print(f"y shape: {y.shape}")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 15)

print(f"X train: {X_train.shape}")
print(f"y train: {y_train.shape}")
print(f"X test: {X_test.shape}")
print(f"y test: {y_test.shape}")

print(f"X train max value: {X_train.max()}")
print(f"X test max value: {X_test.max()}")

scaler = MinMaxScaler()
X_train_scl = scaler.fit_transform(X_train)
X_test_scl = scaler.transform(X_test)

#Saving the scaler model
pickle.dump(scaler, open('scaler.pkl', 'wb'))

"""***Random Forest***"""

#Fitting scaled X_train and y_train on Random Forest Classifier
model_rf = RandomForestClassifier()
model_rf.fit(X_train_scl, y_train)

#Accuracy of the model on training and testing data

print("Training Accuracy :", model_rf.score(X_train_scl, y_train))
print("Testing Accuracy :", model_rf.score(X_test_scl, y_test))

#Predicting on the test set
y_preds = model_rf.predict(X_test_scl)

#Confusion Matrix
cm = confusion_matrix(y_test, y_preds)

cm_display = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model_rf.classes_)
cm_display.plot()
plt.show()

"""**K fold cross-validation**"""

accuracies = cross_val_score(estimator = model_rf, X = X_train_scl, y = y_train, cv = 10)
print("Accuracy :", accuracies.mean())
print("Standard Variance :", accuracies.std())

#Saving the random_forest classifier
pickle.dump(model_rf, open('model_rf.pkl', 'wb'))

params = {
    'bootstrap': [True],
    'max_depth': [80, 100],
    'min_samples_split': [8, 12],
    'n_estimators': [100, 300]
}

cv_object = StratifiedKFold(n_splits = 2)

grid_search = GridSearchCV(estimator = model_rf, param_grid = params, cv = cv_object, verbose = 0, return_train_score = True)
grid_search.fit(X_train_scl, y_train.ravel())

#Getting the best parameters from the grid search
print("Best Parameter Combination : {}".format(grid_search.best_params_))

print("Cross validation mean accuracy on train set : {}".format(grid_search.cv_results_['mean_train_score'].mean()*100))
print("Cross validation mean accuracy on test set : {}".format(grid_search.cv_results_['mean_test_score'].mean()*100))
print("Accuracy score for test set :", accuracy_score(y_test, y_preds))

"""**Using this, you can improve model performance and check how accurately it stabilizes on new data, identifying the best parameters for your XGBClassifier model.**"""

model_xgb = XGBClassifier()
model_xgb.fit(X_train_scl, y_train)

#Accuracy of the model on training and testing data

print("Training Accuracy :", model_xgb.score(X_train_scl, y_train))
print("Testing Accuracy :", model_xgb.score(X_test_scl, y_test))

y_preds = model_xgb.predict(X_test)

#Confusion Matrix
cm = confusion_matrix(y_test, y_preds)
print(cm)

cm_display = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model_xgb.classes_)
cm_display.plot()
plt.show()

#Saving the XGBoost classifier
pickle.dump(model_xgb, open('model_xgb.pkl', 'wb'))

"""**Decision Tree Classifier**"""

model_dt = DecisionTreeClassifier()
model_dt.fit(X_train_scl, y_train)

#Accuracy of the model on training and testing data

print("Training Accuracy :", model_dt.score(X_train_scl, y_train))
print("Testing Accuracy :", model_dt.score(X_test_scl, y_test))

#Saving the Decision_tree classifier
pickle.dump(model_xgb, open('model_dt.pkl', 'wb'))

y_preds = model_dt.predict(X_test)

#Confusion Matrix
cm = confusion_matrix(y_test, y_preds)
print(cm)

cm_display = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model_dt.classes_)
cm_display.plot()
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile index.html
# 
# <!DOCTYPE html>
# <html>
# 
# <head>
#   <meta charset="utf-8" />
#   <meta name="viewport" content="width=device-width, initial-scale=1" />
#   <meta name="theme-color" content="#000000" />
#   <link rel="shortcut icon" href="./assets/img/favicon.ico" />
#   <link rel="apple-touch-icon" sizes="76x76" href="./assets/img/apple-icon.png" />
#   <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" />
#   <link rel="stylesheet"
#     href="https://cdn.jsdelivr.net/gh/creativetimofficial/tailwind-starter-kit/compiled-tailwind.min.css" />
#   <title>Amazon Alexa Reviews Analysis</title>
# </head>
# 
# <body class="text-gray-800 antialiased">
#   <script src="https://cdn.jsdelivr.net/gh/alpinejs/alpine@v2.x.x/dist/alpine.js" defer></script>
# 
#   <main>
#     <div class="relative pt-16 pb-32 flex content-center items-center justify-center" style="min-height: 75vh;">
#       <div class="absolute top-0 w-full h-full bg-center bg-cover"
#         style='background-image: linear-gradient(90deg, rgba(17,223,169,1) 0%, rgba(3,60,88,1) 49%, rgba(10,32,50,1) 100%);'>
#         <span id="blackOverlay" class="w-full h-full absolute opacity-50 bg-black"></span>
#       </div>
#       <div class="container relative mx-auto">
#         <div class="items-center flex flex-wrap">
#           <div class="w-full lg:w-6/12 px-4 ml-auto mr-auto text-center">
#             <div class="pr-12">
#               <h1 class="text-white font-semibold text-5xl">
#                 Understand the emotions behind the words.ðŸ˜Š
#               </h1>
#               <p class="mt-4 text-lg text-gray-300">
#                 Text sentiment prediction is a powerful tool that can help you to understand the emotions and opinions
#                 expressed in your text data.
#                 This information can be used to improve your business in a number of ways
#               </p>
#             </div>
#           </div>
#         </div>
#       </div>
#       <div class="top-auto bottom-0 left-0 right-0 w-full absolute pointer-events-none overflow-hidden"
#         style="height: 70px;">
#         <svg class="absolute bottom-0 overflow-hidden" xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="none"
#           version="1.1" viewBox="0 0 2560 100" x="0" y="0">
#           <polygon class="text-gray-300 fill-current" points="2560 0 2560 100 0 100"></polygon>
#         </svg>
#       </div>
#     </div>
# 
#     <section class="relative py-20">
#       <div class="bottom-auto top-0 left-0 right-0 w-full absolute pointer-events-none overflow-hidden -mt-20"
#         style="height: 80px;">
#         <svg class="absolute bottom-0 overflow-hidden" xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="none"
#           version="1.1" viewBox="0 0 2560 100" x="0" y="0">
#           <polygon class="text-white fill-current" points="2560 0 2560 100 0 100"></polygon>
#         </svg>
#       </div>
#       <div class="flex flex-col items-center justify-center">
#         <h1 class="text-5xl font-semibold  py-2 ">Text Sentiment Prediction</h1>
#       </div>
#       <div class="container mx-auto px-4 py-24">
# 
#         <div class="items-center flex flex-wrap">
#           <div class="w-full md:w-4/12 ml-auto mr-auto px-4">
#             <form id="predictionForm">
# 
# 
# 
# 
#               <div class="container max-w-full mx-auto md:py-24 px-6">
#                 <div class="max-w-sm mx-auto px-6">
#                   <div class="relative flex flex-wrap">
#                     <div class="w-full relative">
#                       <div class="md:mt-6">
#                         <form class="mt-8" id="predictionForm">
#                           <div class="mx-auto max-w-lg ">
#                             <div class="py-1">
#                               <span class="px-1 text-sm text-gray-600">Upload your CSV file</span>
#                               <input placeholder="" type="file" id="csvFileInput" accept=".csv"
#                                 class="text-md block px-3 py-2 rounded-lg w-full
#                                    bg-white border-2 border-gray-300 placeholder-gray-600 shadow-md focus:placeholder-gray-500 focus:bg-white focus:border-gray-600 focus:outline-none">
#                             </div>
# 
#                             <div class="py-1">
#                               <span class="px-1 text-sm text-gray-600">Text for Prediction</span>
#                               <textarea
#                                 class="text-md block px-3 py-2 rounded-lg w-full
#                                 bg-white border-2 border-gray-300 placeholder-gray-600 shadow-md focus:placeholder-gray-500 focus:bg-white focus:border-gray-600 focus:outline-none"
#                                 id="textInput" placeholder="Enter text..."></textarea>
# 
#                             </div>
# 
#                             <button type="button" onclick="predict()" class="mt-3 text-lg font-semibold
#                                                             bg-gray-800 w-full text-white rounded-lg
#                                                      px-6 py-3 block shadow-xl hover:text-white hover:bg-black">
#                               Predict
#                             </button>
# 
#                           </div>
#                         </form>
# 
# 
# 
#                       </div>
#                     </div>
#                   </div>
#                 </div>
#               </div>
#             </form>
#           </div>
#           <div class="w-full md:w-4/12 ml-auto mr-auto px-4">
#             <div>
#               <h1 class="text-4xl underline underline-offset-8  ">Prediction Result</h1>
#               <div class="p-4 m-2 border">
#                 <div id="predictionResult"></div>
# 
#               </div>
#             </div>
#             <div class="pt-6">
#               <h1 class="text-4xl underline underline-offset-8  ">graph Result</h1>
#               <div class="p-4 m-2 border  ">
#                 <div id="graphContainer"></div>
#               </div>
#             </div>
#             <button id="downloadBtn" style="display:none" onclick="downloadPredictions()" class="mt-3 text-lg font-semibold
#             bg-gray-800 w-full text-white rounded-lg
#      px-6 py-3 mt-10 block shadow-xl hover:text-white hover:bg-black">
#               Download Predictions
#             </button>
#           </div>
# 
#         </div>
#       </div>
#     </section>
# 
# 
# 
#   </main>
#   <footer class="relative bg-gray-900 pt-8 pb-6">
#     <div class="bottom-auto top-0 left-0 right-0 w-full absolute pointer-events-none overflow-hidden -mt-20"
#       style="height: 80px;">
#       <svg class="absolute bottom-0 overflow-hidden" xmlns="http://www.w3.org/2000/svg" preserveAspectRatio="none"
#         version="1.1" viewBox="0 0 2560 100" x="0" y="0">
#         <polygon class="text-gray-300 fill-current" points="2560 0 2560 100 0 100"></polygon>
#       </svg>
#     </div>
#     <div class="container mx-auto px-4">
# 
#       <hr class="my-6 border-gray-400" />
#       <div class="flex flex-wrap items-center md:justify-between justify-center">
#         <div class="w-full md:w-4/12 px-4 mx-auto text-center">
#           <div class="text-sm text-white font-semibold py-1">
#             Copyright Â© Text sentiment prediction
# 
#           </div>
#         </div>
#       </div>
#     </div>
#   </footer>
# </body>
# <script>
#   function predict() {
#     // Check if CSV file is present
#     var csvFileInput = document.getElementById("csvFileInput");
#     var textInput = document.getElementById("textInput");
#     var predictionResult = document.getElementById("predictionResult");
#     var graphContainer = document.getElementById("graphContainer");
# 
#     if (csvFileInput.files.length > 0) {
#       // Upload CSV file
#       var formData = new FormData();
#       formData.append("file", csvFileInput.files[0]);
# 
#       fetch("http://localhost:5000/predict", {
#         method: "POST",
#         body: formData
#       })
#         .then(response => {
#           if (response.headers.get('X-Graph-Exists') === 'true') {
#             console.log("Graph")
#             var graphData = response.headers.get('X-Graph-Data');
#             displayGraph(graphData);
#           }
# 
#           return response.blob();
#         })
#         .then(blob => {
#           console.log("Blob:", blob);
# 
#           document.getElementById("downloadBtn").style.display = "block";
#           document.getElementById("downloadBtn").onclick = function () {
#             console.log("Downloading...");
#             var url = URL.createObjectURL(blob);
#             console.log("URL:", url);
# 
#             var a = document.createElement("a");
#             a.href = url;
#             a.download = "Predictions.csv";
#             document.body.appendChild(a);
#             a.click();
#             document.body.removeChild(a);
#           };
#         })
#         .catch(error => {
#           console.error("Error:", error);
#         });
# 
#     } else if (textInput.value.trim() !== "") {
#       // Predict on single sentence
#       fetch("http://localhost:5000/predict", {
#         method: "POST",
#         headers: {
#           "Content-Type": "application/json"
#         },
#         body: JSON.stringify({ "text": textInput.value.trim() })
#       })
#         .then(response => response.json())
#         .then(data => {
#           console.log(data)
#           predictionResult.innerHTML = "Predicted sentiment: " + data.prediction;
#         });
#     }
#   }
# 
#   function downloadPredictions() {
#     console.log("Download prediction")
#   }
# 
#   function displayGraph(graphData) {
#     predictionResult.innerHTML = "";
#     var graphUrl = "data:image/png;base64," + graphData;
#     var img = document.createElement('img');
#     img.src = graphUrl;
#     graphContainer.appendChild(img);
#   }
# </script>
# 
# </html>

from google.colab.output import eval_js
print(eval_js("google.colab.kernel.proxyPort(5000)"))

from flask import Flask, request, jsonify, send_file, render_template
import re
from io import BytesIO
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
import matplotlib.pyplot as plt
import pandas as pd
import pickle
import base64

STOPWORDS = set(stopwords.words("english"))

app = Flask(__name__)

@app.route("/test", methods=["GET"])
def test():
    return "Test request received successfully. Service is running."

@app.route("/", methods=["GET", "POST"])
def home():
     return render_template("landing.html")

@app.route("/predict", methods=["POST"])
def predict():
    predictor = pickle.load(open("/content/model_xgb.pkl", "rb"))
    scaler = pickle.load(open("/content/scaler.pkl", "rb"))
    cv = pickle.load(open("/content/countVectorizer.pkl", "rb"))
    try:
        if "file" in request.files:
            file = request.files["file"]
            data = pd.read_csv(file)
            predictions, graph = bulk_prediction(predictor, scaler, cv, data)
            response = send_file(
                predictions,
                mimetype="text/csv",
                as_attachment=True,
                download_name="/content/Predictions.csv",
            )
            response.headers["X-Graph-Exists"] = "true"
            response.headers["X-Graph-Data"] = base64.b64encode(
                graph.getbuffer()
            ).decode("ascii")
            return response

        elif "text" in request.json:
            text_input = request.json["text"]
            predicted_sentiment = single_prediction(predictor, scaler, cv, text_input)
            return jsonify({"prediction": predicted_sentiment})

    except Exception as e:
        return jsonify({"error": str(e)})

def single_prediction(predictor, scaler, cv, text_input):
    corpus = []
    stemmer = PorterStemmer()
    review = re.sub("[^a-zA-Z]", " ", text_input)
    review = review.lower().split()
    review = [stemmer.stem(word) for word in review if not word in STOPWORDS]
    review = " ".join(review)
    corpus.append(review)
    X_prediction = cv.transform(corpus).toarray()
    X_prediction_scl = scaler.transform(X_prediction)
    y_predictions = predictor.predict_proba(X_prediction_scl)
    y_predictions = y_predictions.argmax(axis=1)[0]
    return "Positive" if y_predictions == 1 else "Negative"

def bulk_prediction(predictor, scaler, cv, data):
    corpus = []
    stemmer = PorterStemmer()
    for i in range(0, data.shape[0]):
        review = re.sub("[^a-zA-Z]", " ", data.iloc[i]["Sentence"])
        review = review.lower().split()
        review = [stemmer.stem(word) for word in review if not word in STOPWORDS]
        review = " ".join(review)
        corpus.append(review)
    X_prediction = cv.transform(corpus).toarray()
    X_prediction_scl = scaler.transform(X_prediction)
    y_predictions = predictor.predict_proba(X_prediction_scl)
    y_predictions = y_predictions.argmax(axis=1)
    y_predictions = list(map(sentiment_mapping, y_predictions))
    data["Predicted sentiment"] = y_predictions
    predictions_csv = BytesIO()
    data.to_csv(predictions_csv, index=False)
    predictions_csv.seek(0)
    graph = get_distribution_graph(data)
    return predictions_csv, graph

def get_distribution_graph(data):
    fig = plt.figure(figsize=(5, 5))
    colors = ("green", "red")
    wp = {"linewidth": 1, "edgecolor": "black"}
    tags = data["Predicted sentiment"].value_counts()
    explode = (0.01, 0.01)
    tags.plot(
        kind="pie",
        autopct="%1.1f%%",
        shadow=True,
        colors=colors,
        startangle=90,
        wedgeprops=wp,
        explode=explode,
        title="Sentiment Distribution",
        xlabel="",
        ylabel="",
    )
    graph = BytesIO()
    plt.savefig(graph, format="png")
    plt.close()
    return graph

def sentiment_mapping(x):
    if x == 1:
        return "Positive"
    else:
        return "Negative"

if __name__ == "__main__":
    app.run(port=5000, debug=True)

!pip install gradio
!pip install nltk pandas scikit-learn xgboost wordcloud matplotlib seaborn

import gradio as gr
import pandas as pd
import pickle
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import MinMaxScaler
from xgboost import XGBClassifier
import matplotlib.pyplot as plt

# Load models and vectorizer
model_rf = pickle.load(open('model_rf.pkl', 'rb'))
model_xgb = pickle.load(open('model_xgb.pkl', 'rb'))
model_dt = pickle.load(open('model_dt.pkl', 'rb'))
scaler = pickle.load(open('scaler.pkl', 'rb'))
cv = pickle.load(open('countVectorizer.pkl', 'rb'))

# Define preprocessing function
def preprocess_text(text):
    stemmer = PorterStemmer()
    STOPWORDS = set(stopwords.words('english'))
    review = re.sub('[^a-zA-Z]', ' ', text)
    review = review.lower().split()
    review = [stemmer.stem(word) for word in review if not word in STOPWORDS]
    review = ' '.join(review)
    return review

# Define prediction function for single text input
def predict_text(text):
    preprocessed_text = preprocess_text(text)
    vectorized_text = cv.transform([preprocessed_text]).toarray()
    scaled_text = scaler.transform(vectorized_text)
    prediction = model_xgb.predict(scaled_text)[0]
    prediction_proba = model_xgb.predict_proba(scaled_text)[0]
    result = "Positive" if prediction == 1 else "Negative"

    # Create a bar chart
    labels = ['Negative', 'Positive']
    fig, ax = plt.subplots()
    ax.bar(labels, prediction_proba, color=['red', 'green'])
    ax.set_ylim(0, 1)
    for i, v in enumerate(prediction_proba):
        ax.text(i, v + 0.01, f"{v:.2f}", ha='center')
    plt.title("Prediction Confidence")
    plt.ylabel("Probability")

    return result, fig

# Define prediction function for CSV input
def predict_csv(file):
    data = pd.read_csv(file.name)
    data['preprocessed'] = data['verified_reviews'].apply(preprocess_text)
    vectorized_data = cv.transform(data['preprocessed']).toarray()
    scaled_data = scaler.transform(vectorized_data)
    predictions = model_xgb.predict(scaled_data)
    data['prediction'] = predictions
    data['prediction'] = data['prediction'].apply(lambda x: "Positive" if x == 1 else "Negative")
    return data

# Create Gradio interface using Blocks
with gr.Blocks() as demo:
    gr.Markdown("## Text Sentiment Prediction")
    with gr.Tab("Single Text Input"):
        text_input = gr.Textbox(lines=2, placeholder="Enter text for sentiment prediction")
        text_output = gr.Textbox(label="Predicted Sentiment")
        plot_output = gr.Plot(label="Prediction Confidence")
        text_button = gr.Button("Predict")
        text_button.click(fn=predict_text, inputs=text_input, outputs=[text_output, plot_output])

    with gr.Tab("CSV Input"):
        csv_input = gr.File(label="Upload CSV file for sentiment prediction")
        csv_output = gr.Dataframe(label="Predictions")
        csv_button = gr.Button("Predict")
        csv_button.click(fn=predict_csv, inputs=csv_input, outputs=csv_output)

demo.launch()

